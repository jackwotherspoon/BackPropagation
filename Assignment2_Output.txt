Assignment 2 Output File 

Student: Jack Wotherspoon 
Student Number: 20012060 

Structure of network is (9,8,6)
As seen 9 input nodes were used, one for each feature given in CSV file.
The number of hidden layers is 1 and it has 8 hidden nodes. 
Only one hidden layer was chosen through trial and error of viewing accuracy on validation set when changing number of layers. Adding a second layer slightly increased accuracy but greatly increased computation and thus was rejected.
The 8 hidden nodes was determined through same trial and error of viewing accuracy on validation set. The 8 nodes gave best results.
There are 6 output nodes, this was chosen due to the number of glass types given which is 6.

Initial Weights:
Input to hidden layer weights:[{'weights': [0.09842010622235076, 0.5939368555169189, 0.5555634713000843, 0.1490480997011865, 0.6274685681261171, 0.9569794384394223, 0.5998118841732917, 0.5982832145359093, 0.6346399079373809, 0.7802180486582796]}, {'weights': [0.34400431407464227, 0.5209361363317991, 0.06056167783060862, 0.9437602448300482, 0.4648779262951809, 0.613395943477919, 0.13774868762621006, 0.42180752006908895, 0.8704449257230318, 0.09284504604133448]}, {'weights': [0.7061744928284682, 0.7778843944934845, 0.9819750196299991, 0.5102701534025673, 0.46868468416023257, 0.9719399466788392, 0.6333340679261811, 0.7078490574491662, 0.6996849360721934, 0.48043945179270353]}, {'weights': [0.7828047637878526, 0.6916292408489353, 0.7756419453563191, 0.0691593505163931, 0.22900288375628042, 0.26845400581637724, 0.8424741923576639, 0.5239135997008478, 0.35793117341621994, 0.9231797407798755]}, {'weights': [0.7337674422825394, 0.42356190486720724, 0.9961376017960676, 0.12001751637381153, 0.3320788031954446, 0.2875065485482968, 0.35795296394346154, 0.38755443341025164, 0.19681382532822778, 0.39531745275360053]}, {'weights': [0.43351784952717387, 0.3815920627415086, 0.2651491917990739, 0.5738959519404244, 0.5538191825293703, 0.867112213358572, 0.8437365928149545, 0.5458117359385509, 0.4415336916276431, 0.6383724309456165]}, {'weights': [0.5332235447518828, 0.99325872060246, 0.0746281139150542, 0.5661729392674234, 0.8482372511791243, 0.14406465005158864, 0.590097026826953, 0.4411871988056041, 0.06644275547101852, 0.1724829905260271]}, {'weights': [0.9352626773849968, 0.3182189217888456, 0.744269774396988, 0.7438199806742004, 0.9104944126572193, 0.0677809084236558, 0.830093075175549, 0.7758039476972237, 0.39696017797240846, 0.7402381619878151]}]
Hidden to output layer weights:[{'weights': [0.44725590436521434, 0.5235631009074047, 0.7866383012980975, 0.7196793763364799, 0.27843157637748916, 0.3577806603407563, 0.31647238108212095, 0.20869584010497222, 0.9663385411281106]}, {'weights': [0.5824735136635414, 0.6470715421945352, 0.25034511156887174, 0.8775368994813395, 0.3917514860802265, 0.9650463477409749, 0.728074697223659, 0.7541193109320508, 0.9203812178722345]}, {'weights': [0.29079729289503975, 0.568821934825929, 0.3444793551190438, 0.45051914704483675, 0.39007484020172645, 0.022453218267120545, 0.00337613741694065, 0.4837006408846649, 0.22410961069319624]}, {'weights': [0.5323575379668437, 0.9918410939464231, 0.7597463524046557, 0.4909366745361854, 0.4977085604057174, 0.21599637991879128, 0.512045127365128, 0.5469455886318274, 0.14406734823634537]}, {'weights': [0.7620264774713946, 0.4040746885607325, 0.933998655368354, 0.7684914506353991, 0.6299469887512272, 0.6587825437838939, 0.2849669182893595, 0.5585232539536582, 0.02514951106469865]}, {'weights': [0.15612113408368233, 0.952762119069965, 0.7984922870562836, 0.8706715484043894, 0.7512394652942782, 0.993963283974735, 0.01876631046645305, 0.20554735119593337, 0.836542879947765]}]

Node Output Function Used: 
Output node function used for this assignment is sigmoid function as it allows for a smooth and bounded function for the total input. It also has the added benefit of having nice derivatives which make learning the weights of a neural network easier.

Learning rate: 0.2
Learning rate was chosen to be 0.2 by using it as a parameter for my validation testing. Through many trials and iterations it was determined to be the best for accuracy. 

Terminating Critera: 
Terminating criteria is one of two things. Training stops when the 1000 epochs are completed or when sum-squared error on training data begins to change super slowly. 
The number of epochs was used as a parameter for validation set, 1000 was determined to be the cutoff as to where increasing the number of epochs seeemed to no longer improve accuracy by a noticeable amount.
Sum-squared error is checked each epoch and if the change in error from one epoch to the next is less than 0.01 than it stops training. This was a parameter for validation testing and was determined through trials and iterations.
This terminating criteria allows the training to not overfit the data by stopping it once it has learned sufficiently.

Momentum value: 0.9
This momentum value was determined through using it as a validation parameter. It allowed my training accuracy to go from 65% to roughly 80% on my training data, probably escaping a local minima. This then transfered to a better validation accuracy. 

Data Preprocessing:
The first part of my preprocessing was to convert the CSV input data from strings to floats and the class column from string to integer. This allows for much easier usage of the data further along.
The main preprocessing I did was normalizing the data. Since the raw data for features vary in scale, some are in range 0-1 and some as high as being in the seventies. This would cause the feature with values in the seventies to alter the node outputs way more than the others.
 Normalizing the data fixes this. I normalized my data into the range of 0-1 as it is good practice to normalize input values to the range of the transfer function which is sigmoid and outputs between 0-1.

Data Splitting
As seen in my split_data() function I chose to split my data into 70% training data, 15% validation data, and 15% testing data.
I chose these splits because you always want the majority of data to be in the training set where it can learn and update weights and biases.
My validation set was given 15% in order to allow me to tune parameters of my network. Using my validation set I was able to tune my number of hidden layers and nodes, learning rate, number of epochs, and terminating criteria and improve validation accuracy with each.
The remaining 15% was for the test set which allowed for an unbiased evaluation of the final model. Only used once the validation set was sufficiently trained (using training and validation set).

Final Weight Vectors:
[[{'weights': [-0.8230934925362865, -3.881838207588764, -1.5040852714479542, 6.901534714622139, -4.9519838311738615, 3.105470679570482, 2.1959063211182412, 6.00471092392046, 5.10541598892532, 0.6776997099687999], 'output': 0.23854495847685556, 'delta': -0.07896483352100478}, {'weights': [5.6487186085064645, -6.198957033344118, -4.851054902904456, 6.424678482491799, -5.6604135260626816, -1.2411745145145932, 7.794814255234133, -1.909685422531073, 0.5402335938357165, -0.9048636097800192], 'output': 0.013397675050005218, 'delta': 0.008896503639168442}, {'weights': [3.5499763800875423, -1.1851432571049367, -0.1042197127298427, -5.923688052360435, -0.8975612020417856, 4.08409372268302, 2.0701873493174467, 2.48171449771547, 2.8190277260368193, -1.9289978357060062], 'output': 0.025910213653174984, 'delta': 0.03025980177601199}, {'weights': [6.79630680966934, -0.009677353384214204, 6.8829434906145694, -11.418378370257013, 0.3525408390886751, 2.187211390522549, 0.2508114679406298, 1.2506842502624331, 5.420482690014139, -1.4077946709643803], 'output': 0.8038075635733389, 'delta': -0.015802781571526685}, {'weights': [9.58330085422176, 4.068756150891695, 9.749462426318594, -13.280674288515833, -2.482651186263264, -0.8994056751176747, 2.3049622041692444, 0.7158207414253237, -14.137948377551156, -5.877474824246601], 'output': 0.2509106868192476, 'delta': -0.001561020218097701}, {'weights': [-0.7349374032165276, -10.030283228318762, -2.5310451289525635, 2.57762701267289, 8.61975951195978, 4.805644532457864, 0.9008664748513239, 1.5036823517465858, 2.4661102290016874, -1.3373771113151243], 'output': 0.543577706294236, 'delta': -0.117151809603152}, {'weights': [-1.7821512125072605, 7.630995575421407, -0.49162768409498375, 5.350236014168209, -2.911794405866507, -3.6022683918025864, -2.6268318126930694, 6.475698705000787, -4.747349644124609, -4.22598525549818], 'output': 0.04390551645427906, 'delta': -0.008082959717643239}, {'weights': [-3.9204285996224835, -1.0213002234147635, 10.119642602785976, 9.015615097989672, -1.4147065437667747, -1.4667868424049841, -8.82629478647634, 8.955997132581379, -7.134423940251929, -5.251095248918094], 'output': 0.8983208391024119, 'delta': 0.16778699362263894}], [{'weights': [2.1120734324812647, -3.05910193569739, 0.33146131755932273, 1.8629780787868782, -1.5058995673881121, -4.75209503543684, -3.106111474569694, 0.7530158103949429, -2.8802702286990756], 'output': 0.034480533109803334, 'delta': -0.0023576612506697576}, {'weights': [-2.4799844430672637, 1.773414548201563, 5.095230778067542, 2.109403403635065, -6.169807544696829, -2.553749822874439, -5.659622725593966, 6.137597896578101, -0.07819566062592809], 'output': 0.9709811215848964, 'delta': 0.11514612860407789}, {'weights': [-6.556383139380678, -3.6141410775607454, -1.7942956941777422, -5.653415158080902, -4.956575871431528, -7.90456587626872, -2.348996377854155, -4.149822781812672, 8.75304635876511], 'output': 0.0010888702238827405, 'delta': -1.318847279720937e-06}, {'weights': [1.2946868455454585, 7.2708133005282685, -5.1777614665352365, -3.4900257174464455, -4.180723374938637, 4.878584341386119, -6.0685381552372695, -2.2303116180558473, -6.258555478470741], 'output': 7.799057077458167e-05, 'delta': -3.503962868321263e-06}, {'weights': [5.943932953986667, -0.03293140968121703, 0.11803493399226557, -3.9151359930084286, -1.6833929568008872, -3.970941686222214, 8.305354149677918, 3.5468637360684223, -6.846708237135252], 'output': 0.0004985858778038703, 'delta': -1.860597759986153e-08}, {'weights': [0.37463459411971634, -4.95490206135678, -1.0065535064889646, 6.898261859435199, 4.335180486273189, 2.8654008696016118, 0.30286293445363743, -3.473882376228063, -8.504712713464057], 'output': 0.03152361726579348, 'delta': -0.0841656844749614}]]
Ignore output and delta part of dictionaries for weights as those were used earlier in code.

Actual Classes: 
[1, 4, 2, 4, 4, 3, 4, 4, 0, 5, 1, 4, 5, 1, 5, 0, 5, 5, 4, 5, 1, 1, 0, 5, 1, 4, 5, 1, 1, 0, 1, 1]

Predicted Classes: 
[1, 5, 5, 4, 4, 3, 4, 4, 1, 1, 1, 4, 1, 1, 5, 1, 1, 5, 4, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 4, 1]
Test set Accuracy: 59.375%

Confusion Matrix:
[[0 4 0 0 0 0]
 [0 9 0 0 1 0]
 [0 0 0 0 0 1]
 [0 0 0 1 0 0]
 [0 0 0 0 7 1]
 [0 6 0 0 0 2]]

Precision and Recall: 
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         4
           1       0.47      0.90      0.62        10
           2       0.00      0.00      0.00         1
           3       1.00      1.00      1.00         1
           4       0.88      0.88      0.88         8
           5       0.50      0.25      0.33         8

   micro avg       0.59      0.59      0.59        32
   macro avg       0.47      0.50      0.47        32
weighted avg       0.52      0.59      0.53        32
